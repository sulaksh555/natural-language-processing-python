{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1ee9c5",
   "metadata": {
    "papermill": {
     "duration": 0.051214,
     "end_time": "2021-12-15T01:07:15.441874",
     "exception": false,
     "start_time": "2021-12-15T01:07:15.390660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b24517",
   "metadata": {
    "papermill": {
     "duration": 0.083704,
     "end_time": "2021-12-15T01:07:15.579561",
     "exception": false,
     "start_time": "2021-12-15T01:07:15.495857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Import the libraries and methods required for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210b22ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:07:15.690947Z",
     "iopub.status.busy": "2021-12-15T01:07:15.690430Z",
     "iopub.status.idle": "2021-12-15T01:08:25.572852Z",
     "shell.execute_reply": "2021-12-15T01:08:25.572165Z",
     "shell.execute_reply.started": "2021-12-14T12:32:30.551027Z"
    },
    "papermill": {
     "duration": 69.943544,
     "end_time": "2021-12-15T01:08:25.572997",
     "exception": false,
     "start_time": "2021-12-15T01:07:15.629453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_text\r\n",
      "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 785 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_text) (0.12.0)\r\n",
      "Collecting tensorflow<2.8,>=2.7.0\r\n",
      "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 489.6 MB 20 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.10.0.2)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.37.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.0)\r\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.38.1)\r\n",
      "Collecting keras<2.8,>=2.7.0rc0\r\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 43.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.0)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.23.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 39.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.19.0)\r\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.19.5)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.12.1)\r\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 463 kB 38.5 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.12)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.14.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.2)\r\n",
      "Collecting libclang>=9.0.1\r\n",
      "  Downloading libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 13.4 MB 39.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tensorboard~=2.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.6.0)\r\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.5.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.8.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.4)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.0.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (58.0.4)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.6.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.25.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.6)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.35.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.7)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.2.2)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.7.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.8)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2021.10.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.5.0)\r\n",
      "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-estimator, libclang, keras, tensorflow, tensorflow-text\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\r\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 2.6.0\r\n",
      "    Uninstalling keras-2.6.0:\r\n",
      "      Successfully uninstalled keras-2.6.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.6.0\r\n",
      "    Uninstalling tensorflow-2.6.0:\r\n",
      "      Successfully uninstalled tensorflow-2.6.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\r\n",
      "tfx-bsl 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.14.0 which is incompatible.\r\n",
      "tfx-bsl 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.3.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.14.0 which is incompatible.\r\n",
      "tensorflow-transform 1.3.0 requires pyarrow<3,>=1, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.3.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2, but you have tensorflow 2.7.0 which is incompatible.\r\n",
      "tensorflow-io 0.18.0 requires tensorflow<2.6.0,>=2.5.0, but you have tensorflow 2.7.0 which is incompatible.\r\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, but you have tensorflow-io-gcs-filesystem 0.23.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed keras-2.7.0 libclang-12.0.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.23.0 tensorflow-text-2.7.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf46dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:26.058262Z",
     "iopub.status.busy": "2021-12-15T01:08:26.057366Z",
     "iopub.status.idle": "2021-12-15T01:08:30.449571Z",
     "shell.execute_reply": "2021-12-15T01:08:30.450048Z",
     "shell.execute_reply.started": "2021-12-14T13:23:40.374312Z"
    },
    "papermill": {
     "duration": 4.638962,
     "end_time": "2021-12-15T01:08:30.450231",
     "exception": false,
     "start_time": "2021-12-15T01:08:25.811269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow_text as tf_text\n",
    "import random\n",
    "import pathlib\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cdb0f",
   "metadata": {
    "papermill": {
     "duration": 0.234475,
     "end_time": "2021-12-15T01:08:30.931607",
     "exception": false,
     "start_time": "2021-12-15T01:08:30.697132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create a shape checker class to ensure that all objects have the right dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9b9933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:31.416844Z",
     "iopub.status.busy": "2021-12-15T01:08:31.415908Z",
     "iopub.status.idle": "2021-12-15T01:08:31.417891Z",
     "shell.execute_reply": "2021-12-15T01:08:31.418294Z",
     "shell.execute_reply.started": "2021-12-12T04:32:36.870263Z"
    },
    "papermill": {
     "duration": 0.246473,
     "end_time": "2021-12-15T01:08:31.418461",
     "exception": false,
     "start_time": "2021-12-15T01:08:31.171988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "        \n",
    "    def __call__(self,\n",
    "                 tensor, \n",
    "                 names,\n",
    "                 broadcast = False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "        \n",
    "        if isinstance(names, str):\n",
    "            names = (names, )\n",
    "            \n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "        \n",
    "        if rank != len(names):\n",
    "            raise ValueError(f\"Rank mismatch:\\n\"\n",
    "                             f\"   Found {rank}: {shape.numpy()}\\n\"\n",
    "                             f\"   Expected {len(names)}: {names}\\n\")\n",
    "            \n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "            \n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "                \n",
    "            if old_dim is None:\n",
    "                # if the axis name is new, add its length to the cache\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "                \n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"   Found: {new_dim}\\n\"\n",
    "                                 f\"   Expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7aeaf",
   "metadata": {
    "papermill": {
     "duration": 0.236302,
     "end_time": "2021-12-15T01:08:31.889998",
     "exception": false,
     "start_time": "2021-12-15T01:08:31.653696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2edeec",
   "metadata": {
    "papermill": {
     "duration": 0.233821,
     "end_time": "2021-12-15T01:08:32.364957",
     "exception": false,
     "start_time": "2021-12-15T01:08:32.131136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8efad6",
   "metadata": {
    "papermill": {
     "duration": 0.23978,
     "end_time": "2021-12-15T01:08:32.838063",
     "exception": false,
     "start_time": "2021-12-15T01:08:32.598283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load the Spanish-to-English dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05833bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:33.325283Z",
     "iopub.status.busy": "2021-12-15T01:08:33.324370Z",
     "iopub.status.idle": "2021-12-15T01:08:33.679102Z",
     "shell.execute_reply": "2021-12-15T01:08:33.678665Z",
     "shell.execute_reply.started": "2021-12-14T12:37:08.268799Z"
    },
    "papermill": {
     "duration": 0.597935,
     "end_time": "2021-12-15T01:08:33.679242",
     "exception": false,
     "start_time": "2021-12-15T01:08:33.081307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 0s 0us/step\n",
      "2654208/2638744 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\"spa-eng.zip\",\n",
    "                                      origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "                                      extract = True)\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/\"spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48261b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:34.235999Z",
     "iopub.status.busy": "2021-12-15T01:08:34.235013Z",
     "iopub.status.idle": "2021-12-15T01:08:34.239199Z",
     "shell.execute_reply": "2021-12-15T01:08:34.238531Z",
     "shell.execute_reply.started": "2021-12-14T12:37:12.312115Z"
    },
    "papermill": {
     "duration": 0.321263,
     "end_time": "2021-12-15T01:08:34.239393",
     "exception": false,
     "start_time": "2021-12-15T01:08:33.918130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding = \"utf-8\")\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split(\"\\t\") for line in lines]\n",
    "    \n",
    "    inp = [inp for inp, targ in pairs]\n",
    "    targ = [targ for inp, targ in pairs]\n",
    "    \n",
    "    return inp, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32199f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:34.917200Z",
     "iopub.status.busy": "2021-12-15T01:08:34.916245Z",
     "iopub.status.idle": "2021-12-15T01:08:35.289769Z",
     "shell.execute_reply": "2021-12-15T01:08:35.290297Z",
     "shell.execute_reply.started": "2021-12-14T12:37:16.280881Z"
    },
    "papermill": {
     "duration": 0.652631,
     "end_time": "2021-12-15T01:08:35.290486",
     "exception": false,
     "start_time": "2021-12-15T01:08:34.637855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp, targ = load_data(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e4d78b",
   "metadata": {
    "papermill": {
     "duration": 0.234912,
     "end_time": "2021-12-15T01:08:35.759220",
     "exception": false,
     "start_time": "2021-12-15T01:08:35.524308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training the model on the full dataset will take a very long time. Subset a desired number of examples from the original dataset to train the model on within a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514fcee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:36.360953Z",
     "iopub.status.busy": "2021-12-15T01:08:36.360037Z",
     "iopub.status.idle": "2021-12-15T01:08:36.363002Z",
     "shell.execute_reply": "2021-12-15T01:08:36.363596Z",
     "shell.execute_reply.started": "2021-12-14T12:37:20.499886Z"
    },
    "papermill": {
     "duration": 0.257971,
     "end_time": "2021-12-15T01:08:36.363776",
     "exception": false,
     "start_time": "2021-12-15T01:08:36.105805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the full dataset: 118964\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of examples in the full dataset: {len(inp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3f63e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:36.833904Z",
     "iopub.status.busy": "2021-12-15T01:08:36.832947Z",
     "iopub.status.idle": "2021-12-15T01:08:36.836167Z",
     "shell.execute_reply": "2021-12-15T01:08:36.836731Z",
     "shell.execute_reply.started": "2021-12-14T12:37:23.995287Z"
    },
    "papermill": {
     "duration": 0.240799,
     "end_time": "2021-12-15T01:08:36.836891",
     "exception": false,
     "start_time": "2021-12-15T01:08:36.596092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_desired_examples = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4313eed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:37.351066Z",
     "iopub.status.busy": "2021-12-15T01:08:37.350208Z",
     "iopub.status.idle": "2021-12-15T01:08:37.354286Z",
     "shell.execute_reply": "2021-12-15T01:08:37.355284Z",
     "shell.execute_reply.started": "2021-12-14T12:37:27.693739Z"
    },
    "papermill": {
     "duration": 0.283945,
     "end_time": "2021-12-15T01:08:37.355528",
     "exception": false,
     "start_time": "2021-12-15T01:08:37.071583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = inp[0:n_desired_examples]\n",
    "targ = targ[0:n_desired_examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3097d10",
   "metadata": {
    "papermill": {
     "duration": 0.231943,
     "end_time": "2021-12-15T01:08:37.832269",
     "exception": false,
     "start_time": "2021-12-15T01:08:37.600326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca170790",
   "metadata": {
    "papermill": {
     "duration": 0.24379,
     "end_time": "2021-12-15T01:08:38.315767",
     "exception": false,
     "start_time": "2021-12-15T01:08:38.071977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Carve out a training set and a test set from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c9049a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:08:38.869385Z",
     "iopub.status.busy": "2021-12-15T01:08:38.843370Z",
     "iopub.status.idle": "2021-12-15T01:09:01.698518Z",
     "shell.execute_reply": "2021-12-15T01:09:01.698009Z"
    },
    "papermill": {
     "duration": 23.144811,
     "end_time": "2021-12-15T01:09:01.698666",
     "exception": false,
     "start_time": "2021-12-15T01:08:38.553855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_indices = list(range(n_desired_examples))\n",
    "train_size = int(0.9 * n_desired_examples)\n",
    "train_indices = random.sample(original_indices, train_size)\n",
    "test_indices = [index for index in original_indices if (index not in train_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee483d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:02.223516Z",
     "iopub.status.busy": "2021-12-15T01:09:02.222491Z",
     "iopub.status.idle": "2021-12-15T01:09:02.224550Z",
     "shell.execute_reply": "2021-12-15T01:09:02.224980Z"
    },
    "papermill": {
     "duration": 0.280024,
     "end_time": "2021-12-15T01:09:02.225133",
     "exception": false,
     "start_time": "2021-12-15T01:09:01.945109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs = [inp[i] for i in train_indices]\n",
    "test_inputs = [inp[i] for i in test_indices]\n",
    "train_targets = [targ[i] for i in train_indices]\n",
    "test_targets = [targ[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc57dd6",
   "metadata": {
    "papermill": {
     "duration": 0.237382,
     "end_time": "2021-12-15T01:09:02.696459",
     "exception": false,
     "start_time": "2021-12-15T01:09:02.459077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c85359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:03.182205Z",
     "iopub.status.busy": "2021-12-15T01:09:03.180534Z",
     "iopub.status.idle": "2021-12-15T01:09:03.182842Z",
     "shell.execute_reply": "2021-12-15T01:09:03.183275Z",
     "shell.execute_reply.started": "2021-12-12T04:33:16.683761Z"
    },
    "papermill": {
     "duration": 0.254424,
     "end_time": "2021-12-15T01:09:03.183461",
     "exception": false,
     "start_time": "2021-12-15T01:09:02.929037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_inputs)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85cd1717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:03.703231Z",
     "iopub.status.busy": "2021-12-15T01:09:03.693061Z",
     "iopub.status.idle": "2021-12-15T01:09:04.194235Z",
     "shell.execute_reply": "2021-12-15T01:09:04.194896Z",
     "shell.execute_reply.started": "2021-12-12T04:33:20.249768Z"
    },
    "papermill": {
     "duration": 0.771618,
     "end_time": "2021-12-15T01:09:04.195115",
     "exception": false,
     "start_time": "2021-12-15T01:09:03.423497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 01:09:03.969780: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-12-15 01:09:04.002920: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_targets)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599c07a",
   "metadata": {
    "papermill": {
     "duration": 0.234285,
     "end_time": "2021-12-15T01:09:04.670376",
     "exception": false,
     "start_time": "2021-12-15T01:09:04.436091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create the translator template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add93008",
   "metadata": {
    "papermill": {
     "duration": 0.271434,
     "end_time": "2021-12-15T01:09:05.176378",
     "exception": false,
     "start_time": "2021-12-15T01:09:04.904944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb1649e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:05.659630Z",
     "iopub.status.busy": "2021-12-15T01:09:05.658874Z",
     "iopub.status.idle": "2021-12-15T01:09:05.661374Z",
     "shell.execute_reply": "2021-12-15T01:09:05.662024Z",
     "shell.execute_reply.started": "2021-12-12T04:33:31.796999Z"
    },
    "papermill": {
     "duration": 0.248489,
     "end_time": "2021-12-15T01:09:05.662197",
     "exception": false,
     "start_time": "2021-12-15T01:09:05.413708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # split accented characters\n",
    "    text = tf_text.normalize_utf8(text,\n",
    "                                  \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "    \n",
    "    # keep spaces, a-z, and select punctuation\n",
    "    text = tf.strings.regex_replace(text,\n",
    "                                    \"[^ a-z.?!,¿]\",\n",
    "                                    \"\")\n",
    "    \n",
    "    # add spaces around punctuation\n",
    "    text = tf.strings.regex_replace(text,\n",
    "                                    \"[.?!,¿]\",\n",
    "                                    r\" \\0 \")\n",
    "    \n",
    "    # strip whitespace\n",
    "    text = tf.strings.strip(text)\n",
    "    \n",
    "    text = tf.strings.join([\"[START]\", text, \"[END]\"],\n",
    "                           separator = \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a65549",
   "metadata": {
    "papermill": {
     "duration": 0.253454,
     "end_time": "2021-12-15T01:09:06.149191",
     "exception": false,
     "start_time": "2021-12-15T01:09:05.895737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b59204a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:06.644876Z",
     "iopub.status.busy": "2021-12-15T01:09:06.643941Z",
     "iopub.status.idle": "2021-12-15T01:09:06.645767Z",
     "shell.execute_reply": "2021-12-15T01:09:06.646228Z",
     "shell.execute_reply.started": "2021-12-12T04:33:37.764516Z"
    },
    "papermill": {
     "duration": 0.251954,
     "end_time": "2021-12-15T01:09:06.646400",
     "exception": false,
     "start_time": "2021-12-15T01:09:06.394446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 input_vocab_size,\n",
    "                 embedding_dim,\n",
    "                 enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        \n",
    "        # the embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = self.input_vocab_size,\n",
    "                                                   output_dim = embedding_dim)\n",
    "        \n",
    "        # the GRU RNN layer processes those vectors sequentially\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       # return the sequence and state\n",
    "                                       return_sequences = True,\n",
    "                                       return_state = True,\n",
    "                                       recurrent_initializer = \"glorot_uniform\")\n",
    "        \n",
    "    def call(self,\n",
    "             tokens, \n",
    "             state = None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, (\"batch\", \"s\"))\n",
    "        \n",
    "        # the embedding layer looks up the embedding for each token\n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, (\"batch\", \"s\", \"embed_dim\"))\n",
    "        \n",
    "        # the GRU processes the embedding sequence\n",
    "        # output shape: (batch, s, enc_units)\n",
    "        # state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors,\n",
    "                                 initial_state = state)\n",
    "        shape_checker(output, (\"batch\", \"s\", \"enc_units\"))\n",
    "        shape_checker(state, (\"batch\", \"enc_units\"))\n",
    "        \n",
    "        # returns the new sequence and its state\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4297a4",
   "metadata": {
    "papermill": {
     "duration": 0.354322,
     "end_time": "2021-12-15T01:09:07.247444",
     "exception": false,
     "start_time": "2021-12-15T01:09:06.893122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b4b3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:07.943579Z",
     "iopub.status.busy": "2021-12-15T01:09:07.941878Z",
     "iopub.status.idle": "2021-12-15T01:09:07.944155Z",
     "shell.execute_reply": "2021-12-15T01:09:07.944590Z",
     "shell.execute_reply.started": "2021-12-12T04:33:43.380966Z"
    },
    "papermill": {
     "duration": 0.249878,
     "end_time": "2021-12-15T01:09:07.944734",
     "exception": false,
     "start_time": "2021-12-15T01:09:07.694856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the attention scoring function in Bahdanau's additive style\n",
    "        self.W1 = tf.keras.layers.Dense(units, \n",
    "                                        use_bias = False)\n",
    "        self.W2 = tf.keras.layers.Dense(units,\n",
    "                                        use_bias = False)\n",
    "        \n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "        \n",
    "    def call(self,\n",
    "             query,\n",
    "             value, \n",
    "             mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, (\"batch\", \"t\", \"query_units\"))\n",
    "        shape_checker(value, (\"batch\", \"s\", \"value_units\"))\n",
    "        shape_checker(mask, (\"batch\", \"s\"))\n",
    "        \n",
    "        # the W1@ht term in the attention score formula\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, (\"batch\", \"t\", \"attn_units\"))\n",
    "        \n",
    "        # the W2@hs term in the attention score formula\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, (\"batch\", \"s\", \"attn_units\"))\n",
    "        \n",
    "        query_mask = tf.ones(tf.shape(query)[:-1],\n",
    "                             dtype = bool)\n",
    "        value_mask = mask\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(inputs = [w1_query, value, w2_key],\n",
    "                                                           mask = [query_mask, value_mask],\n",
    "                                                           return_attention_scores = True)\n",
    "        shape_checker(context_vector, (\"batch\", \"t\", \"value_units\"))\n",
    "        shape_checker(attention_weights, (\"batch\", \"t\", \"s\"))\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9abcbc",
   "metadata": {
    "papermill": {
     "duration": 0.235635,
     "end_time": "2021-12-15T01:09:08.416897",
     "exception": false,
     "start_time": "2021-12-15T01:09:08.181262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46ffd47a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:08.895832Z",
     "iopub.status.busy": "2021-12-15T01:09:08.895024Z",
     "iopub.status.idle": "2021-12-15T01:09:08.897716Z",
     "shell.execute_reply": "2021-12-15T01:09:08.897257Z",
     "shell.execute_reply.started": "2021-12-12T04:33:49.099386Z"
    },
    "papermill": {
     "duration": 0.24677,
     "end_time": "2021-12-15T01:09:08.897834",
     "exception": false,
     "start_time": "2021-12-15T01:09:08.651064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_vocab_size, \n",
    "                 embedding_dim,\n",
    "                 dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # step 1 - the embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = self.output_vocab_size,\n",
    "                                                   output_dim = embedding_dim)\n",
    "        \n",
    "        # step 2 - the RNN keeps track of what's been generated so far\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences = True,\n",
    "                                       return_state = True,\n",
    "                                       recurrent_initializer = \"glorot_uniform\")\n",
    "        \n",
    "        # step 3 - the RNN output will be the query for the attention layer\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "        # step 4 - converting the context vector to the attention vector\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units,\n",
    "                                        activation = tf.math.tanh,\n",
    "                                        use_bias = False)\n",
    "        \n",
    "        # step 5 - this fully connected layer produces the logits for each output token\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b642700d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:09.378930Z",
     "iopub.status.busy": "2021-12-15T01:09:09.378314Z",
     "iopub.status.idle": "2021-12-15T01:09:09.382292Z",
     "shell.execute_reply": "2021-12-15T01:09:09.381854Z",
     "shell.execute_reply.started": "2021-12-12T04:33:54.215723Z"
    },
    "papermill": {
     "duration": 0.250947,
     "end_time": "2021-12-15T01:09:09.382431",
     "exception": false,
     "start_time": "2021-12-15T01:09:09.131484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb11b9d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:09.867980Z",
     "iopub.status.busy": "2021-12-15T01:09:09.867065Z",
     "iopub.status.idle": "2021-12-15T01:09:09.869145Z",
     "shell.execute_reply": "2021-12-15T01:09:09.869546Z",
     "shell.execute_reply.started": "2021-12-12T04:33:57.810663Z"
    },
    "papermill": {
     "duration": 0.252269,
     "end_time": "2021-12-15T01:09:09.869690",
     "exception": false,
     "start_time": "2021-12-15T01:09:09.617421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call(self,\n",
    "         inputs: DecoderInput,\n",
    "         state = None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(inputs.new_tokens, (\"batch\", \"t\"))\n",
    "    shape_checker(inputs.enc_output, (\"batch\", \"s\", \"enc_units\"))\n",
    "    shape_checker(inputs.mask, (\"batch\", \"s\"))\n",
    "    \n",
    "    if state is not None:\n",
    "        shape_checker(state, (\"batch\", \"dec_units\"))\n",
    "        \n",
    "    # step 1 - lookup the embeddings\n",
    "    vectors = self.embedding(inputs.new_tokens)\n",
    "    shape_checker(vectors, (\"batch\", \"t\", \"embedding_dim\"))\n",
    "    \n",
    "    # step 2 - process one step with the RNN\n",
    "    rnn_output, state = self.gru(vectors,\n",
    "                                 initial_state = state)\n",
    "    \n",
    "    shape_checker(rnn_output, (\"batch\", \"t\", \"dec_units\"))\n",
    "    shape_checker(state, (\"batch\", \"dec_units\"))\n",
    "    \n",
    "    # step 3 - use the RNN output as the query for the attention over the encoder output\n",
    "    context_vector, attention_weights = self.attention(query = rnn_output,\n",
    "                                                       value = inputs.enc_output,\n",
    "                                                       mask = inputs.mask)\n",
    "    shape_checker(context_vector, (\"batch\", \"t\", \"dec_units\"))\n",
    "    shape_checker(attention_weights, (\"batch\", \"t\", \"s\"))\n",
    "    \n",
    "    # step 4 - join the context_vector and rnn_output\n",
    "    # [ct; ht] shape: (batch t, value_units + query_units)\n",
    "    context_and_rnn_output = tf.concat([context_vector, rnn_output],\n",
    "                                       axis = -1)\n",
    "    \n",
    "    # step 4 (continued) - at = tanh(Wc@[ct; ht])\n",
    "    attention_vector = self.Wc(context_and_rnn_output)\n",
    "    shape_checker(attention_vector, (\"batch\", \"t\", \"dec_units\"))\n",
    "    \n",
    "    # step 5 - generate logit predictions\n",
    "    logits = self.fc(attention_vector)\n",
    "    shape_checker(logits, (\"batch\", \"t\", \"output_vocab_size\"))\n",
    "    \n",
    "    return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b4be641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:10.362513Z",
     "iopub.status.busy": "2021-12-15T01:09:10.361536Z",
     "iopub.status.idle": "2021-12-15T01:09:10.364017Z",
     "shell.execute_reply": "2021-12-15T01:09:10.363504Z",
     "shell.execute_reply.started": "2021-12-12T04:34:03.452634Z"
    },
    "papermill": {
     "duration": 0.261554,
     "end_time": "2021-12-15T01:09:10.364145",
     "exception": false,
     "start_time": "2021-12-15T01:09:10.102591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Decoder.call = call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a61844",
   "metadata": {
    "papermill": {
     "duration": 0.236817,
     "end_time": "2021-12-15T01:09:10.839537",
     "exception": false,
     "start_time": "2021-12-15T01:09:10.602720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "642307a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:11.322727Z",
     "iopub.status.busy": "2021-12-15T01:09:11.322036Z",
     "iopub.status.idle": "2021-12-15T01:09:11.325522Z",
     "shell.execute_reply": "2021-12-15T01:09:11.325003Z",
     "shell.execute_reply.started": "2021-12-12T04:34:07.228467Z"
    },
    "papermill": {
     "duration": 0.251472,
     "end_time": "2021-12-15T01:09:11.325685",
     "exception": false,
     "start_time": "2021-12-15T01:09:11.074213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = \"masked_loss\"\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n",
    "                                                                  reduction = \"none\")\n",
    "        \n",
    "    def __call__(self,\n",
    "                 y_true,\n",
    "                 y_pred):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(y_true, (\"batch\", \"t\"))\n",
    "        shape_checker(y_pred, (\"batch\", \"t\", \"logits\"))\n",
    "        \n",
    "        # calculate the loss for each item in the batch\n",
    "        loss = self.loss(y_true,\n",
    "                         y_pred)\n",
    "        shape_checker(loss, (\"batch\", \"t\"))\n",
    "        \n",
    "        # mask off the losses on padding\n",
    "        mask = tf.cast(y_true != 0,\n",
    "                       tf.float32)\n",
    "        shape_checker(mask, (\"batch\", \"t\"))\n",
    "        loss *= mask\n",
    "        \n",
    "        # return the total\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db28d0",
   "metadata": {
    "papermill": {
     "duration": 0.237496,
     "end_time": "2021-12-15T01:09:11.796659",
     "exception": false,
     "start_time": "2021-12-15T01:09:11.559163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implementing the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23e9b0b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:12.283415Z",
     "iopub.status.busy": "2021-12-15T01:09:12.282372Z",
     "iopub.status.idle": "2021-12-15T01:09:12.285310Z",
     "shell.execute_reply": "2021-12-15T01:09:12.285744Z",
     "shell.execute_reply.started": "2021-12-12T04:34:13.033106Z"
    },
    "papermill": {
     "duration": 0.254164,
     "end_time": "2021-12-15T01:09:12.285909",
     "exception": false,
     "start_time": "2021-12-15T01:09:12.031745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 units,\n",
    "                 input_text_processor,\n",
    "                 output_text_processor,\n",
    "                 use_tf_function = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                          embedding_dim,\n",
    "                          units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                          embedding_dim,\n",
    "                          units)\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        \n",
    "    def train_step(self,\n",
    "                   inputs):\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55e2091c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:12.908754Z",
     "iopub.status.busy": "2021-12-15T01:09:12.907889Z",
     "iopub.status.idle": "2021-12-15T01:09:12.914058Z",
     "shell.execute_reply": "2021-12-15T01:09:12.914545Z",
     "shell.execute_reply.started": "2021-12-12T04:34:18.085748Z"
    },
    "papermill": {
     "duration": 0.245104,
     "end_time": "2021-12-15T01:09:12.914701",
     "exception": false,
     "start_time": "2021-12-15T01:09:12.669597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _preprocess(self, \n",
    "                input_text, \n",
    "                target_text):\n",
    "    self.shape_checker(input_text, (\"batch\", ))\n",
    "    self.shape_checker(target_text, (\"batch\", ))\n",
    "    \n",
    "    # convert the text to token IDs\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    target_tokens = self.output_text_processor(target_text)\n",
    "    self.shape_checker(input_tokens, (\"batch\", \"s\"))\n",
    "    self.shape_checker(target_tokens, (\"batch\", \"t\"))\n",
    "\n",
    "    # convert IDs to masks\n",
    "    input_mask = input_tokens != 0\n",
    "    self.shape_checker(input_mask, (\"batch\", \"s\"))\n",
    "\n",
    "    target_mask = target_tokens != 0\n",
    "    self.shape_checker(target_mask, (\"batch\", \"t\"))\n",
    "\n",
    "    return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5928b5ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:13.397891Z",
     "iopub.status.busy": "2021-12-15T01:09:13.397036Z",
     "iopub.status.idle": "2021-12-15T01:09:13.398926Z",
     "shell.execute_reply": "2021-12-15T01:09:13.399370Z",
     "shell.execute_reply.started": "2021-12-12T04:34:22.118082Z"
    },
    "papermill": {
     "duration": 0.252156,
     "end_time": "2021-12-15T01:09:13.399520",
     "exception": false,
     "start_time": "2021-12-15T01:09:13.147364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d367f16b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:13.873241Z",
     "iopub.status.busy": "2021-12-15T01:09:13.866714Z",
     "iopub.status.idle": "2021-12-15T01:09:13.875867Z",
     "shell.execute_reply": "2021-12-15T01:09:13.875325Z",
     "shell.execute_reply.started": "2021-12-12T04:34:25.375591Z"
    },
    "papermill": {
     "duration": 0.246093,
     "end_time": "2021-12-15T01:09:13.875994",
     "exception": false,
     "start_time": "2021-12-15T01:09:13.629901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _train_step(self, \n",
    "                inputs):\n",
    "    input_text, target_text = inputs  \n",
    "    (input_tokens, input_mask, target_tokens, target_mask) = self._preprocess(input_text, \n",
    "                                                                              target_text)\n",
    "\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # encode the input\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "        self.shape_checker(enc_output, (\"batch\", \"s\", \"enc_units\"))\n",
    "        self.shape_checker(enc_state, (\"batch\", \"enc_units\"))\n",
    "\n",
    "        # initialize the decoder's state to the encoder's final state\n",
    "        # this only works if the encoder and decoder have the same number of units\n",
    "        dec_state = enc_state\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for t in tf.range(max_target_length-1):\n",
    "            # pass in two tokens from the target sequence:\n",
    "            # 1. the current input to the decoder.\n",
    "            # 2. the target for the decoder's next prediction.\n",
    "            new_tokens = target_tokens[:, t:t+2]\n",
    "            step_loss, dec_state = self._loop_step(new_tokens, \n",
    "                                                   input_mask,\n",
    "                                                   enc_output, \n",
    "                                                   dec_state)\n",
    "            loss = loss + step_loss\n",
    "\n",
    "        # average the loss over all non padding tokens.\n",
    "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "    # apply an optimization step\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # return a dict mapping metric names to current value\n",
    "    return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8db89d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:14.361496Z",
     "iopub.status.busy": "2021-12-15T01:09:14.360502Z",
     "iopub.status.idle": "2021-12-15T01:09:14.362160Z",
     "shell.execute_reply": "2021-12-15T01:09:14.362587Z",
     "shell.execute_reply.started": "2021-12-12T04:34:31.015559Z"
    },
    "papermill": {
     "duration": 0.251388,
     "end_time": "2021-12-15T01:09:14.362732",
     "exception": false,
     "start_time": "2021-12-15T01:09:14.111344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e45aed65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:14.839268Z",
     "iopub.status.busy": "2021-12-15T01:09:14.838390Z",
     "iopub.status.idle": "2021-12-15T01:09:14.845523Z",
     "shell.execute_reply": "2021-12-15T01:09:14.846494Z",
     "shell.execute_reply.started": "2021-12-12T04:34:35.701235Z"
    },
    "papermill": {
     "duration": 0.250749,
     "end_time": "2021-12-15T01:09:14.846653",
     "exception": false,
     "start_time": "2021-12-15T01:09:14.595904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "    # run the decoder one step\n",
    "    decoder_input = DecoderInput(new_tokens = input_token,\n",
    "                                 enc_output = enc_output,\n",
    "                                 mask = input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state = dec_state)\n",
    "    self.shape_checker(dec_result.logits, (\"batch\", \"t1\", \"logits\"))\n",
    "    self.shape_checker(dec_result.attention_weights, (\"batch\", \"t1\", \"s\"))\n",
    "    self.shape_checker(dec_state, (\"batch\", \"dec_units\"))\n",
    "\n",
    "    # 'self.loss' returns the total for non-padded tokens\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1507925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:15.344365Z",
     "iopub.status.busy": "2021-12-15T01:09:15.343350Z",
     "iopub.status.idle": "2021-12-15T01:09:15.345064Z",
     "shell.execute_reply": "2021-12-15T01:09:15.345494Z",
     "shell.execute_reply.started": "2021-12-12T04:34:39.938197Z"
    },
    "papermill": {
     "duration": 0.254966,
     "end_time": "2021-12-15T01:09:15.345640",
     "exception": false,
     "start_time": "2021-12-15T01:09:15.090674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcb42d",
   "metadata": {
    "papermill": {
     "duration": 0.234424,
     "end_time": "2021-12-15T01:09:15.809105",
     "exception": false,
     "start_time": "2021-12-15T01:09:15.574681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f979a6ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:16.307252Z",
     "iopub.status.busy": "2021-12-15T01:09:16.306225Z",
     "iopub.status.idle": "2021-12-15T01:09:16.308064Z",
     "shell.execute_reply": "2021-12-15T01:09:16.308524Z",
     "shell.execute_reply.started": "2021-12-12T04:34:43.843345Z"
    },
    "papermill": {
     "duration": 0.258751,
     "end_time": "2021-12-15T01:09:16.308683",
     "exception": false,
     "start_time": "2021-12-15T01:09:16.049932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature = [[tf.TensorSpec(dtype = tf.string,\n",
    "                                               shape = [None]),\n",
    "                                 tf.TensorSpec(dtype = tf.string,\n",
    "                                               shape = [None])]])\n",
    "\n",
    "def _tf_train_step(self, inputs):\n",
    "    return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8931ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:16.787267Z",
     "iopub.status.busy": "2021-12-15T01:09:16.786334Z",
     "iopub.status.idle": "2021-12-15T01:09:16.787923Z",
     "shell.execute_reply": "2021-12-15T01:09:16.788327Z",
     "shell.execute_reply.started": "2021-12-12T04:34:48.818743Z"
    },
    "papermill": {
     "duration": 0.248699,
     "end_time": "2021-12-15T01:09:16.788491",
     "exception": false,
     "start_time": "2021-12-15T01:09:16.539792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainTranslator._tf_train_step = _tf_train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29092f44",
   "metadata": {
    "papermill": {
     "duration": 0.252865,
     "end_time": "2021-12-15T01:09:17.276307",
     "exception": false,
     "start_time": "2021-12-15T01:09:17.023442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef91bfc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:17.917634Z",
     "iopub.status.busy": "2021-12-15T01:09:17.916865Z",
     "iopub.status.idle": "2021-12-15T01:09:17.919617Z",
     "shell.execute_reply": "2021-12-15T01:09:17.919152Z",
     "shell.execute_reply.started": "2021-12-12T04:34:53.61668Z"
    },
    "papermill": {
     "duration": 0.247154,
     "end_time": "2021-12-15T01:09:17.919743",
     "exception": false,
     "start_time": "2021-12-15T01:09:17.672589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d47d97",
   "metadata": {
    "papermill": {
     "duration": 0.406754,
     "end_time": "2021-12-15T01:09:18.620952",
     "exception": false,
     "start_time": "2021-12-15T01:09:18.214198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d9e029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:19.128250Z",
     "iopub.status.busy": "2021-12-15T01:09:19.127130Z",
     "iopub.status.idle": "2021-12-15T01:09:19.129254Z",
     "shell.execute_reply": "2021-12-15T01:09:19.129706Z",
     "shell.execute_reply.started": "2021-12-12T04:34:57.869186Z"
    },
    "papermill": {
     "duration": 0.248319,
     "end_time": "2021-12-15T01:09:19.129885",
     "exception": false,
     "start_time": "2021-12-15T01:09:18.881566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 input_text_processor,\n",
    "                 output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        self.output_token_string_from_index = (\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary = output_text_processor.get_vocabulary(),\n",
    "                mask_token = \"\",\n",
    "                invert = True))\n",
    "\n",
    "        # the output should never generate padding, unknown, or start\n",
    "        index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "            vocabulary = output_text_processor.get_vocabulary(), \n",
    "            mask_token = \"\")\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], \n",
    "                           dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b64f6",
   "metadata": {
    "papermill": {
     "duration": 0.238811,
     "end_time": "2021-12-15T01:09:19.607060",
     "exception": false,
     "start_time": "2021-12-15T01:09:19.368249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convert token IDs to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15476fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:20.084559Z",
     "iopub.status.busy": "2021-12-15T01:09:20.083942Z",
     "iopub.status.idle": "2021-12-15T01:09:20.087638Z",
     "shell.execute_reply": "2021-12-15T01:09:20.087137Z",
     "shell.execute_reply.started": "2021-12-12T04:35:03.500779Z"
    },
    "papermill": {
     "duration": 0.245495,
     "end_time": "2021-12-15T01:09:20.087767",
     "exception": false,
     "start_time": "2021-12-15T01:09:19.842272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(result_tokens, (\"batch\", \"t\"))\n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "    shape_checker(result_text_tokens, (\"batch\", \"t\"))\n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                         axis=1, \n",
    "                                         separator=\" \")\n",
    "    shape_checker(result_text, (\"batch\"))\n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    shape_checker(result_text, (\"batch\", ))\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a50cdbd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:20.566170Z",
     "iopub.status.busy": "2021-12-15T01:09:20.565495Z",
     "iopub.status.idle": "2021-12-15T01:09:20.567141Z",
     "shell.execute_reply": "2021-12-15T01:09:20.566695Z",
     "shell.execute_reply.started": "2021-12-12T04:35:07.327064Z"
    },
    "papermill": {
     "duration": 0.244815,
     "end_time": "2021-12-15T01:09:20.567264",
     "exception": false,
     "start_time": "2021-12-15T01:09:20.322449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614774e",
   "metadata": {
    "papermill": {
     "duration": 0.233469,
     "end_time": "2021-12-15T01:09:21.038198",
     "exception": false,
     "start_time": "2021-12-15T01:09:20.804729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sampling from the decoder's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d17013a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:21.528891Z",
     "iopub.status.busy": "2021-12-15T01:09:21.527172Z",
     "iopub.status.idle": "2021-12-15T01:09:21.529706Z",
     "shell.execute_reply": "2021-12-15T01:09:21.530142Z",
     "shell.execute_reply.started": "2021-12-12T04:35:11.39648Z"
    },
    "papermill": {
     "duration": 0.252301,
     "end_time": "2021-12-15T01:09:21.530298",
     "exception": false,
     "start_time": "2021-12-15T01:09:21.277997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(self, logits, temperature):\n",
    "    shape_checker = ShapeChecker()\n",
    "    # 't' is usually 1 here\n",
    "    shape_checker(logits, (\"batch\", \"t\", \"vocab\"))\n",
    "    shape_checker(self.token_mask, (\"vocab\", ))\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "    shape_checker(token_mask, (\"batch\", \"t\", \"vocab\"), \n",
    "                  broadcast = True)\n",
    "\n",
    "    # set the logits for all masked tokens to -inf, so they are never chosen\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        new_tokens = tf.argmax(logits, \n",
    "                               axis = -1)\n",
    "    else: \n",
    "        logits = tf.squeeze(logits, \n",
    "                            axis = 1)\n",
    "    new_tokens = tf.random.categorical(logits / temperature,\n",
    "                                       num_samples=1)\n",
    "\n",
    "    shape_checker(new_tokens, (\"batch\", \"t\"))\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e604b285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:22.005017Z",
     "iopub.status.busy": "2021-12-15T01:09:22.004076Z",
     "iopub.status.idle": "2021-12-15T01:09:22.005681Z",
     "shell.execute_reply": "2021-12-15T01:09:22.006118Z",
     "shell.execute_reply.started": "2021-12-12T04:35:16.014422Z"
    },
    "papermill": {
     "duration": 0.240741,
     "end_time": "2021-12-15T01:09:22.006258",
     "exception": false,
     "start_time": "2021-12-15T01:09:21.765517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Translator.sample = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2fd90f",
   "metadata": {
    "papermill": {
     "duration": 0.238196,
     "end_time": "2021-12-15T01:09:22.476319",
     "exception": false,
     "start_time": "2021-12-15T01:09:22.238123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implementing the translation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95bfa30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:23.136789Z",
     "iopub.status.busy": "2021-12-15T01:09:23.135943Z",
     "iopub.status.idle": "2021-12-15T01:09:23.138077Z",
     "shell.execute_reply": "2021-12-15T01:09:23.138491Z",
     "shell.execute_reply.started": "2021-12-12T04:35:20.113436Z"
    },
    "papermill": {
     "duration": 0.265957,
     "end_time": "2021-12-15T01:09:23.138644",
     "exception": false,
     "start_time": "2021-12-15T01:09:22.872687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_unrolled(self,\n",
    "                       input_text, \n",
    "                       *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "    batch_size = tf.shape(input_text)[0]\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "    dec_state = enc_state\n",
    "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "    result_tokens = []\n",
    "    attention = []\n",
    "    done = tf.zeros([batch_size, 1], \n",
    "                    dtype = tf.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        dec_input = DecoderInput(new_tokens = new_tokens,\n",
    "                                 enc_output = enc_output,\n",
    "                                 mask = (input_tokens != 0))\n",
    "\n",
    "        dec_result, dec_state = self.decoder(dec_input, \n",
    "                                             state = dec_state)\n",
    "\n",
    "        attention.append(dec_result.attention_weights)\n",
    "\n",
    "        new_tokens = self.sample(dec_result.logits, \n",
    "                                 temperature)\n",
    "\n",
    "        # if a sequence produces an 'end_token', set it 'done'\n",
    "        done = done | (new_tokens == self.end_token)\n",
    "        # once a sequence is done it only produces 0-padding\n",
    "        new_tokens = tf.where(done, \n",
    "                              tf.constant(0, dtype = tf.int64), \n",
    "                              new_tokens)\n",
    "\n",
    "        # collect the generated tokens\n",
    "        result_tokens.append(new_tokens)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # convert the list of generates token ids to a list of strings\n",
    "    result_tokens = tf.concat(result_tokens,\n",
    "                              axis = -1)\n",
    "    result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "    if return_attention:\n",
    "        attention_stack = tf.concat(attention, \n",
    "                                    axis = 1)\n",
    "        return {\"text\": result_text, \n",
    "                \"attention\": attention_stack}\n",
    "    else:\n",
    "        return {\"text\": result_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1872c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:23.613840Z",
     "iopub.status.busy": "2021-12-15T01:09:23.612760Z",
     "iopub.status.idle": "2021-12-15T01:09:23.614726Z",
     "shell.execute_reply": "2021-12-15T01:09:23.615201Z",
     "shell.execute_reply.started": "2021-12-12T04:35:25.715098Z"
    },
    "papermill": {
     "duration": 0.242843,
     "end_time": "2021-12-15T01:09:23.615346",
     "exception": false,
     "start_time": "2021-12-15T01:09:23.372503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Translator.translate = translate_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79ffc194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:24.090964Z",
     "iopub.status.busy": "2021-12-15T01:09:24.090025Z",
     "iopub.status.idle": "2021-12-15T01:09:24.092343Z",
     "shell.execute_reply": "2021-12-15T01:09:24.092809Z",
     "shell.execute_reply.started": "2021-12-12T04:35:29.275049Z"
    },
    "papermill": {
     "duration": 0.244409,
     "end_time": "2021-12-15T01:09:24.092984",
     "exception": false,
     "start_time": "2021-12-15T01:09:23.848575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature = [tf.TensorSpec(dtype = tf.string, \n",
    "                                              shape = [None])])\n",
    "def tf_translate(self, input_text):\n",
    "    return self.translate(input_text)\n",
    "\n",
    "Translator.tf_translate = tf_translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3990005",
   "metadata": {
    "papermill": {
     "duration": 0.234152,
     "end_time": "2021-12-15T01:09:24.563193",
     "exception": false,
     "start_time": "2021-12-15T01:09:24.329041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build and train a translator model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02d634",
   "metadata": {
    "papermill": {
     "duration": 0.235722,
     "end_time": "2021-12-15T01:09:25.033602",
     "exception": false,
     "start_time": "2021-12-15T01:09:24.797880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae954b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:25.523700Z",
     "iopub.status.busy": "2021-12-15T01:09:25.522754Z",
     "iopub.status.idle": "2021-12-15T01:09:25.524359Z",
     "shell.execute_reply": "2021-12-15T01:09:25.524774Z",
     "shell.execute_reply.started": "2021-12-12T04:35:32.961197Z"
    },
    "papermill": {
     "duration": 0.247764,
     "end_time": "2021-12-15T01:09:25.524914",
     "exception": false,
     "start_time": "2021-12-15T01:09:25.277150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eee2db29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:26.023395Z",
     "iopub.status.busy": "2021-12-15T01:09:26.022402Z",
     "iopub.status.idle": "2021-12-15T01:09:28.023279Z",
     "shell.execute_reply": "2021-12-15T01:09:28.023877Z",
     "shell.execute_reply.started": "2021-12-12T04:35:36.764549Z"
    },
    "papermill": {
     "duration": 2.260021,
     "end_time": "2021-12-15T01:09:28.024112",
     "exception": false,
     "start_time": "2021-12-15T01:09:25.764091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text_processor = preprocessing.TextVectorization(standardize = tf_lower_and_split_punct,\n",
    "                                                       max_tokens = max_vocab_size)\n",
    "input_text_processor.adapt(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4438549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:28.581439Z",
     "iopub.status.busy": "2021-12-15T01:09:28.579256Z",
     "iopub.status.idle": "2021-12-15T01:09:31.471892Z",
     "shell.execute_reply": "2021-12-15T01:09:31.471381Z",
     "shell.execute_reply.started": "2021-12-12T04:35:40.264572Z"
    },
    "papermill": {
     "duration": 3.159923,
     "end_time": "2021-12-15T01:09:31.472026",
     "exception": false,
     "start_time": "2021-12-15T01:09:28.312103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_text_processor = preprocessing.TextVectorization(standardize = tf_lower_and_split_punct,\n",
    "                                                        max_tokens = max_vocab_size)\n",
    "output_text_processor.adapt(train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2f028",
   "metadata": {
    "papermill": {
     "duration": 0.2331,
     "end_time": "2021-12-15T01:09:31.938628",
     "exception": false,
     "start_time": "2021-12-15T01:09:31.705528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Compile and train a translator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6259eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:32.421781Z",
     "iopub.status.busy": "2021-12-15T01:09:32.420909Z",
     "iopub.status.idle": "2021-12-15T01:09:32.422739Z",
     "shell.execute_reply": "2021-12-15T01:09:32.423166Z",
     "shell.execute_reply.started": "2021-12-12T04:35:43.715188Z"
    },
    "papermill": {
     "duration": 0.245756,
     "end_time": "2021-12-15T01:09:32.423311",
     "exception": false,
     "start_time": "2021-12-15T01:09:32.177555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d5f518a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:32.896447Z",
     "iopub.status.busy": "2021-12-15T01:09:32.895528Z",
     "iopub.status.idle": "2021-12-15T01:09:32.921006Z",
     "shell.execute_reply": "2021-12-15T01:09:32.921444Z",
     "shell.execute_reply.started": "2021-12-12T04:35:47.783243Z"
    },
    "papermill": {
     "duration": 0.263766,
     "end_time": "2021-12-15T01:09:32.921608",
     "exception": false,
     "start_time": "2021-12-15T01:09:32.657842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(embedding_dim, \n",
    "                                   units,\n",
    "                                   input_text_processor = input_text_processor,\n",
    "                                   output_text_processor = output_text_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96b57f67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:33.401983Z",
     "iopub.status.busy": "2021-12-15T01:09:33.401029Z",
     "iopub.status.idle": "2021-12-15T01:09:33.408897Z",
     "shell.execute_reply": "2021-12-15T01:09:33.409339Z",
     "shell.execute_reply.started": "2021-12-12T04:35:51.702581Z"
    },
    "papermill": {
     "duration": 0.252955,
     "end_time": "2021-12-15T01:09:33.409551",
     "exception": false,
     "start_time": "2021-12-15T01:09:33.156596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_translator.compile(loss = MaskedLoss(),\n",
    "                         optimizer = tf.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4160280f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:33.901364Z",
     "iopub.status.busy": "2021-12-15T01:09:33.900425Z",
     "iopub.status.idle": "2021-12-15T01:09:33.902780Z",
     "shell.execute_reply": "2021-12-15T01:09:33.902227Z",
     "shell.execute_reply.started": "2021-12-12T04:35:55.048144Z"
    },
    "papermill": {
     "duration": 0.241836,
     "end_time": "2021-12-15T01:09:33.902899",
     "exception": false,
     "start_time": "2021-12-15T01:09:33.661063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_loss = BatchLogs(\"batch_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "947d5dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T01:09:34.382280Z",
     "iopub.status.busy": "2021-12-15T01:09:34.381334Z",
     "iopub.status.idle": "2021-12-15T02:26:47.230143Z",
     "shell.execute_reply": "2021-12-15T02:26:47.230592Z",
     "shell.execute_reply.started": "2021-12-12T04:35:58.797301Z"
    },
    "papermill": {
     "duration": 4633.089453,
     "end_time": "2021-12-15T02:26:47.230764",
     "exception": false,
     "start_time": "2021-12-15T01:09:34.141311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "704/704 [==============================] - 1528s 2s/step - batch_loss: 2.6977\n",
      "Epoch 2/3\n",
      "704/704 [==============================] - 1523s 2s/step - batch_loss: 1.2905\n",
      "Epoch 3/3\n",
      "704/704 [==============================] - 1521s 2s/step - batch_loss: 0.8837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56980d1950>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_translator.fit(dataset, \n",
    "                     epochs = 3,\n",
    "                     callbacks = [batch_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d064961",
   "metadata": {
    "papermill": {
     "duration": 0.773966,
     "end_time": "2021-12-15T02:26:48.804097",
     "exception": false,
     "start_time": "2021-12-15T02:26:48.030131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Use the trained model to create a translator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7faa7969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:26:50.386469Z",
     "iopub.status.busy": "2021-12-15T02:26:50.385646Z",
     "iopub.status.idle": "2021-12-15T02:26:50.519637Z",
     "shell.execute_reply": "2021-12-15T02:26:50.519148Z",
     "shell.execute_reply.started": "2021-12-12T04:36:34.994918Z"
    },
    "papermill": {
     "duration": 0.929983,
     "end_time": "2021-12-15T02:26:50.519776",
     "exception": false,
     "start_time": "2021-12-15T02:26:49.589793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator = Translator(encoder = train_translator.encoder,\n",
    "                        decoder = train_translator.decoder,\n",
    "                        input_text_processor = input_text_processor,\n",
    "                        output_text_processor = output_text_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684eb0e",
   "metadata": {
    "papermill": {
     "duration": 0.877078,
     "end_time": "2021-12-15T02:26:52.434008",
     "exception": false,
     "start_time": "2021-12-15T02:26:51.556930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad7fa2",
   "metadata": {
    "papermill": {
     "duration": 0.775939,
     "end_time": "2021-12-15T02:26:53.988901",
     "exception": false,
     "start_time": "2021-12-15T02:26:53.212962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Evaluate the accuracy of the model's translations by computing its BLEU (Bilingual Evaluation Understudy) score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bd9401d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:26:55.552796Z",
     "iopub.status.busy": "2021-12-15T02:26:55.552053Z",
     "iopub.status.idle": "2021-12-15T02:26:55.555656Z",
     "shell.execute_reply": "2021-12-15T02:26:55.555164Z",
     "shell.execute_reply.started": "2021-12-14T12:51:25.247625Z"
    },
    "papermill": {
     "duration": 0.790736,
     "end_time": "2021-12-15T02:26:55.555788",
     "exception": false,
     "start_time": "2021-12-15T02:26:54.765052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def StandardizeTarget(target):\n",
    "    # split accented characters\n",
    "    target = tf_text.normalize_utf8(target,\n",
    "                                    \"NFKD\")\n",
    "    target = tf.strings.lower(target)\n",
    "    \n",
    "    # keep spaces, a-z, and select punctuation\n",
    "    target = tf.strings.regex_replace(target,\n",
    "                                      \"[^ a-z.?!,¿]\",\n",
    "                                      \"\")\n",
    "    \n",
    "    # add spaces around punctuation\n",
    "    target = tf.strings.regex_replace(target,\n",
    "                                      \"[.?!,¿]\",\n",
    "                                      r\" \\0 \")\n",
    "    \n",
    "    # strip whitespace\n",
    "    target = tf.strings.strip(target)\n",
    "    \n",
    "    # convert the string tensor to a regular string\n",
    "    target = target.numpy().decode()\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2666f34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:26:57.282972Z",
     "iopub.status.busy": "2021-12-15T02:26:57.281398Z",
     "iopub.status.idle": "2021-12-15T02:26:57.283720Z",
     "shell.execute_reply": "2021-12-15T02:26:57.282192Z",
     "shell.execute_reply.started": "2021-12-14T12:51:06.020815Z"
    },
    "papermill": {
     "duration": 0.960362,
     "end_time": "2021-12-15T02:26:57.283888",
     "exception": false,
     "start_time": "2021-12-15T02:26:56.323526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def StandardizeListOfTargets(list_of_targets):\n",
    "    return [StandardizeTarget(target) for target in list_of_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d491d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:26:59.113445Z",
     "iopub.status.busy": "2021-12-15T02:26:59.112542Z",
     "iopub.status.idle": "2021-12-15T02:26:59.114570Z",
     "shell.execute_reply": "2021-12-15T02:26:59.114945Z",
     "shell.execute_reply.started": "2021-12-14T12:51:09.129194Z"
    },
    "papermill": {
     "duration": 0.777637,
     "end_time": "2021-12-15T02:26:59.115090",
     "exception": false,
     "start_time": "2021-12-15T02:26:58.337453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TokenizeListOfTargets(list_of_targets):\n",
    "    return [target.split(\" \") for target in list_of_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "227acea7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:27:00.658236Z",
     "iopub.status.busy": "2021-12-15T02:27:00.657441Z",
     "iopub.status.idle": "2021-12-15T02:28:05.743107Z",
     "shell.execute_reply": "2021-12-15T02:28:05.742126Z",
     "shell.execute_reply.started": "2021-12-12T04:36:47.165479Z"
    },
    "papermill": {
     "duration": 65.859317,
     "end_time": "2021-12-15T02:28:05.743260",
     "exception": false,
     "start_time": "2021-12-15T02:26:59.883943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "\n",
    "input_text = tf.constant(test_inputs)\n",
    "raw_predictions = translator.translate(input_text = input_text)\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(input_text)):\n",
    "    predictions.append(raw_predictions[\"text\"][i].numpy().decode())\n",
    "\n",
    "tokenized_predictions = list(map(lambda x: x.split(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8982382a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:28:07.321006Z",
     "iopub.status.busy": "2021-12-15T02:28:07.320149Z",
     "iopub.status.idle": "2021-12-15T02:28:08.259594Z",
     "shell.execute_reply": "2021-12-15T02:28:08.260351Z",
     "shell.execute_reply.started": "2021-12-14T12:51:31.057794Z"
    },
    "papermill": {
     "duration": 1.742784,
     "end_time": "2021-12-15T02:28:08.260707",
     "exception": false,
     "start_time": "2021-12-15T02:28:06.517923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collate the targets into a list of list of list of tokens\n",
    "\n",
    "# make a mapping from each input text to all target texts which it corresponds to\n",
    "input_to_target = dict()\n",
    "\n",
    "for input_text, target_text in list(zip(test_inputs, test_targets)):\n",
    "    if input_text not in input_to_target.keys():\n",
    "        input_to_target[input_text] = [target_text]\n",
    "    else:\n",
    "        input_to_target[input_text] = input_to_target[input_text] + [target_text]\n",
    "\n",
    "# cluster together the alternative target texts corresponding to each input text\n",
    "clustered_targets = list(map(input_to_target.get, test_inputs))\n",
    "\n",
    "# standardize the target text\n",
    "standardized_targets = [StandardizeListOfTargets(list_of_targets) for list_of_targets in clustered_targets]\n",
    "\n",
    "# tokenize the target text\n",
    "tokenized_targets = [TokenizeListOfTargets(list_of_targets) for list_of_targets in standardized_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5c96d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:28:10.256510Z",
     "iopub.status.busy": "2021-12-15T02:28:10.231027Z",
     "iopub.status.idle": "2021-12-15T02:28:10.590824Z",
     "shell.execute_reply": "2021-12-15T02:28:10.591485Z",
     "shell.execute_reply.started": "2021-12-12T04:37:00.929102Z"
    },
    "papermill": {
     "duration": 1.292706,
     "end_time": "2021-12-15T02:28:10.591702",
     "exception": false,
     "start_time": "2021-12-15T02:28:09.298996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for the final model: 0.23347675470460474\n"
     ]
    }
   ],
   "source": [
    "# obtain the BLEU score across all predictions\n",
    "\n",
    "bleu_score = corpus_bleu(list_of_references = tokenized_targets,\n",
    "                         hypotheses = tokenized_predictions)\n",
    "\n",
    "print(f\"BLEU score for the final model: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65593f2c",
   "metadata": {
    "papermill": {
     "duration": 0.775596,
     "end_time": "2021-12-15T02:28:12.143476",
     "exception": false,
     "start_time": "2021-12-15T02:28:11.367880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Export the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd38b74",
   "metadata": {
    "papermill": {
     "duration": 0.770946,
     "end_time": "2021-12-15T02:28:13.679533",
     "exception": false,
     "start_time": "2021-12-15T02:28:12.908587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Save the architecture and weights associated with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac4bf9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T02:28:15.232154Z",
     "iopub.status.busy": "2021-12-15T02:28:15.231311Z",
     "iopub.status.idle": "2021-12-15T02:28:36.829823Z",
     "shell.execute_reply": "2021-12-15T02:28:36.829231Z",
     "shell.execute_reply.started": "2021-12-12T04:37:09.555312Z"
    },
    "papermill": {
     "duration": 22.378379,
     "end_time": "2021-12-15T02:28:36.829972",
     "exception": false,
     "start_time": "2021-12-15T02:28:14.451593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 02:28:30.358279: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(translator, \n",
    "                    \"translator\",\n",
    "                    signatures = {\"serving_default\": translator.tf_translate})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4893.632446,
   "end_time": "2021-12-15T02:28:41.047662",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-15T01:07:07.415216",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
